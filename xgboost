def objective(trial):

    max_d = trial.suggest_int('max_d', 2, 10)
    lr = trial.suggest_loguniform('lr', 0.001, 0.1)
    booster = 'dart'
    gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)
    min_child_weight = trial.suggest_loguniform('child_w', 0.1, 10.0)
    #max_delta_step 
    subsample = trial.suggest_discrete_uniform('subsample', 0.6, 0.95, 0.05)
    max_f = trial.suggest_discrete_uniform('max_f', 0.6, 0.95, 0.05)
    #colsample_bylevel
    #colsample_bynode
    #reg_alpha
    #reg_lambda
    
    #dart param
    sample_type = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])
    normalize_type = trial.suggest_categorical('normalize_type', ['tree', 'forest'])
    rate_drop = trial.suggest_loguniform('r_drop', 1e-8, 1.0)
    #one_drop
    skip_drop = trial.suggest_loguniform('s_drop', 1e-8, 1.0)
    
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        verbosity=0,
        n_estimators=1000,
        max_depth = max_d,
        learning_late = lr,
        gamma = gamma,
        min_child_weight = min_child_weight,
        subsample = subsample,
        colsample_bytree = max_f,
        sample_type = sample_type,
        normalize_type = normalize_type,
        rate_drop = rate_drop,
        skip_drop = skip_drop,
        random_state = 0
    )
    
    score = cross_val_score(model, x_train, y_train, cv=5)
    score = score.mean()

    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=200)
