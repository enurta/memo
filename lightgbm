def objective(trial):
    
    lgbm_params = {
        'verbose':-1,
        'objective': 'regression',
        'metric': 'rmse',
        'boosting': 'gbdt',
        #'num_trees': 10,
        'learning_rate': trial.suggest_discrete_uniform('lr', 0.01, 1.0, 0.01),
        'max_leaves': trial.suggest_int('max_leaf', 10, 100),
        'random_state': 0,
        'max_depth': trial.suggest_int('max_d', -1, 10),
        'min_data_in_leaf': trial.suggest_int('min_s', 3, 30),
        'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 0.95, 0.05),
        'bagging_freq': trial.suggest_int('freq', 0, 50),
        'feature_fraction': trial.suggest_discrete_uniform('max_f', 0.01, 1.0, 0.01),
        'max_bin': trial.suggest_int('max_bin', 64, 512),
    }
    
    cv_results = lgb.cv(lgbm_params, lgb_train, nfold=5, stratified=False, num_boost_round=500)
    
#     kf = KFold(5)
    
#     cv_score = []
    
#     for train_idx, val_idx in kf.split(feature_df, train_df):
#         x_trn, x_val = feature_df.iloc[train_idx], feature_df.iloc[val_idx]
#         y_trn, y_val = train_df.iloc[train_idx], train_df.iloc[val_idx]
        
#         y_trn = y_trn.target
#         y_val = y_val.target
        
#         lgb_train = lgb.Dataset(x_trn, y_trn)
#         lgb_eval = lgb.Dataset(x_val, y_val, reference=lgb_train)
        
#         model = lgb.train(lgbm_params, lgb_train, num_boost_round=100)
        
#         pred = model.predict(x_val)
        
#         score = roc_auc_score(y_val, pred)
#         cv_score.append(score)
    
    #print(cv_results)
    score = cv_results['rmse-mean'][-1]
    
    return score

study = optuna.create_study(direction='minimize', sampler=optuna.samplers.RandomSampler(seed=1))
study.optimize(objective, n_trials=1000)
